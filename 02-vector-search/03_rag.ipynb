{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44990327",
   "metadata": {},
   "source": [
    "# RAG with Vector Search using Qdrant\n",
    "\n",
    "This notebook demonstrates two approaches to RAG:\n",
    "1. **Text-based search** using Minsearch (keyword matching)\n",
    "2. **Vector search** using Qdrant (semantic similarity)\n",
    "\n",
    "## Why Vector Search?\n",
    "\n",
    "Traditional keyword search matches exact words, but **vector search** understands meaning:\n",
    "- \"How do I start Kafka?\" and \"Steps to run Kafka\" are semantically similar\n",
    "- Vector embeddings capture this semantic similarity\n",
    "- Better retrieval = better answers from the LLM\n",
    "\n",
    "We'll use:\n",
    "- **Qdrant**: Vector database for storing and searching embeddings\n",
    "- **Jina Embeddings**: Model to convert text into vectors\n",
    "- **Hugging Face Llama**: LLM for generating answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da063f",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the token is loaded\n",
    "token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "print(f\"Token loaded: {token[:10]}...\" if token else \"Token not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4134be8",
   "metadata": {},
   "source": [
    "## Step 2: Load Documents Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8e2ae0-da25-44e7-ae82-024173150a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57de60e5-b96c-499c-a7cf-0f30fc33b324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - Can I still join the course after the start date?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7d0d18-5c07-4010-9f90-bbd021f110c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexe\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x24a2177c320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf066460",
   "metadata": {},
   "source": [
    "## Step 3: Create Text Search Index (Baseline)\n",
    "\n",
    "First, let's set up traditional text-based search as a baseline for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39ccad",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Hugging Face LLM\n",
    "\n",
    "Set up the Llama 3.2 3B model via Hugging Face's Inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc5784e-6515-42e5-be62-8fb915df1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Hugging Face model setup\n",
    "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "client = ChatHuggingFace(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=prompt),\n",
    "        ]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b65a",
   "metadata": {},
   "source": [
    "## Step 5: Test Baseline RAG with Text Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka, you can execute the following command in your project directory for the Java Kafka producer:\\n\\n```\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n```\\n\\nFor Python, ensure that you create a virtual environment and run the necessary dependencies as indicated. First, create the virtual environment and activate it:\\n\\n```\\npython -m venv env\\nsource env/bin/activate  # On MacOS/Linux\\n# or\\nenv\\\\Scripts\\\\activate  # On Windows\\n```\\n\\nThen, install the required packages from `requirements.txt`. After setting up the environment, you can run your Python scripts. Remember that Docker images should be up and running before executing any Python files.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run kafka?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can still enroll in the course even if it has already started. You are also eligible to submit the homework assignments. However, make sure to pay attention to the deadlines for turning in the final projects to avoid leaving everything until the last minute.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71154c38-cc68-4d29-9809-f0f0545c79c3",
   "metadata": {},
   "source": [
    "## Step 6: RAG with Vector Search (Qdrant)\n",
    "\n",
    "Now let's upgrade to **semantic vector search** using Qdrant!\n",
    "\n",
    "### What we'll do:\n",
    "1. Connect to Qdrant (vector database)\n",
    "2. Create embeddings for all documents using Jina model\n",
    "3. Store embeddings in Qdrant\n",
    "4. Search using semantic similarity (not just keywords)\n",
    "5. Use the same LLM to generate answers\n",
    "\n",
    "### Prerequisites:\n",
    "Make sure Qdrant is running in Docker:\n",
    "```bash\n",
    "docker run -d -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"./qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78f5cb1-709c-4b48-a9b9-1738b75415de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c297eca",
   "metadata": {},
   "source": [
    "### Connect to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61784d3a-b8f1-45b2-9404-e3495dca1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f435b8da-834e-45c1-b83e-e28c294c044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643514",
   "metadata": {},
   "source": [
    "### Configure Embedding Model\n",
    "\n",
    "We'll use **Jina Embeddings v2 Small** - a lightweight but powerful embedding model:\n",
    "- Converts text into 512-dimensional vectors\n",
    "- Optimized for semantic similarity\n",
    "- Works well for English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fa85d4-af1d-46b9-a281-87c5f332bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"zoomcamp-faq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5957c-22c2-43e6-8dc1-af0c1acc884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client.delete_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722e75a1-95ab-4388-94c0-800ec4f58866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d5067",
   "metadata": {},
   "source": [
    "### Create Qdrant Collection\n",
    "\n",
    "Create a collection to store our document vectors:\n",
    "- **size=512**: Match the embedding dimensionality\n",
    "- **distance=COSINE**: Use cosine similarity to measure how close vectors are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb29d2a0-64df-4ea2-8920-8e6a16c4bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fb83c",
   "metadata": {},
   "source": [
    "### Create Index for Filtering\n",
    "\n",
    "This allows us to filter by course name efficiently (like SQL's WHERE clause)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1348ef23-364a-4719-94fe-7a9cf8ea8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    vector = models.Document(text=text, model=model_handle)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418fe8d5",
   "metadata": {},
   "source": [
    "### Generate and Store Embeddings\n",
    "\n",
    "For each document:\n",
    "1. Combine question + text\n",
    "2. Generate embedding vector using Jina model\n",
    "3. Store vector + metadata in Qdrant\n",
    "\n",
    "This may take a minute as we process all documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e0e480-2af4-4a0e-b3d0-28a1e2181195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c9be7f7-2813-432b-8d73-3dff448dd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'I just discovered the course. Can I still join it?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6698670a-6fe7-4f51-83f7-281e80e06f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    print('vector_search is used')\n",
    "    \n",
    "    course = 'data-engineering-zoomcamp'\n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        query_filter=models.Filter( \n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8eaa12",
   "metadata": {},
   "source": [
    "### Define Vector Search Function\n",
    "\n",
    "This function performs semantic search:\n",
    "1. Converts the query into a vector using the same Jina model\n",
    "2. Finds the 5 most similar document vectors in Qdrant\n",
    "3. Filters by course name\n",
    "4. Returns the matching documents\n",
    "\n",
    "**Key difference from text search**: This finds semantically similar content, not just keyword matches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d80d18e3-c512-4f97-9f77-b1145fdb73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = vector_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7a18b",
   "metadata": {},
   "source": [
    "### Updated RAG Function with Vector Search\n",
    "\n",
    "Same pipeline as before, but now using semantic vector search instead of keyword search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90bdc6f4-b6f8-4491-84f4-bbd8a6b9f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_search is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To run Kafka, you need to follow these steps based on your scripts:\\n\\n1. Make sure your Kafka broker is running. You can confirm this by running `docker ps`. If the broker is not active, navigate to the folder with your docker-compose yaml file and run `docker compose up -d` to start all instances.\\n\\n2. In your project directory, to run the producer, use the following command:\\n   ```\\n   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n   ```\\n\\n3. Ensure that the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in your Java scripts (e.g., JsonProducer.java, JsonConsumer.java) is set to the correct server URL. Also, verify that the cluster key and secrets in `src/main/java/org/example/Secrets.java` are updated with the correct values.\\n\\nBy following these steps, you should be able to run Kafka successfully.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run kafka?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6524c",
   "metadata": {},
   "source": [
    "### Test Vector Search RAG\n",
    "\n",
    "Try asking questions in different ways - vector search understands the semantic meaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886901ee-6e55-4295-a106-af25e6483da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "Two complete RAG systems:\n",
    "1. **Text-based RAG** - Keyword matching with Minsearch\n",
    "2. **Vector-based RAG** - Semantic search with Qdrant\n",
    "\n",
    "### Text vs Vector Search Comparison\n",
    "\n",
    "| Aspect | Text Search (Minsearch) | Vector Search (Qdrant) |\n",
    "|--------|------------------------|------------------------|\n",
    "| **How it works** | Matches keywords | Measures semantic similarity |\n",
    "| **Query flexibility** | Exact words needed | Understands meaning/paraphrasing |\n",
    "| **Example** | \"run Kafka\" matches \"run Kafka\" | \"start Kafka\" finds \"run Kafka\" |\n",
    "| **Speed** | Very fast | Fast (requires embedding) |\n",
    "| **Setup** | Simple | Requires embedding model + vector DB |\n",
    "| **Best for** | Known keywords | Natural language questions |\n",
    "\n",
    "### When to Use Each?\n",
    "\n",
    "**Use Text Search when:**\n",
    "- Users search with exact terms/codes\n",
    "- Speed is critical\n",
    "- Simple setup is needed\n",
    "\n",
    "**Use Vector Search when:**\n",
    "- Users ask natural language questions\n",
    "- Semantic understanding matters\n",
    "- Handling synonyms/paraphrases is important\n",
    "\n",
    "**Use Both (Hybrid):**\n",
    "- Combine for best results (see `04_hybrid_search.ipynb`)\n",
    "\n",
    "### Key Advantage of Hugging Face Endpoints\n",
    "- **No local GPU required** for the LLM (Llama 3.2)\n",
    "- **Serverless architecture** - pay per use\n",
    "- **Easy model switching** - just change the `repo_id`\n",
    "- **Production-ready** - scales automatically\n",
    "\n",
    "### Next Steps\n",
    "- Explore `04_hybrid_search.ipynb` for combining both approaches\n",
    "- Try different embedding models\n",
    "- Experiment with different LLMs from Hugging Face\n",
    "- Compare answer quality between text and vector RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
