{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "158371d5",
   "metadata": {},
   "source": [
    "# RAG Starter - Building a Question-Answering System\n",
    "\n",
    "This notebook demonstrates how to build a **Retrieval-Augmented Generation (RAG)** system using:\n",
    "- **Hugging Face** for the language model (LLM)\n",
    "- **Minsearch** for document retrieval\n",
    "- **LangChain** for orchestration\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines two key components:\n",
    "1. **Retrieval**: Finding relevant documents from a knowledge base\n",
    "2. **Generation**: Using an LLM to generate answers based on the retrieved context\n",
    "\n",
    "This approach allows the LLM to provide accurate, context-aware answers without having all knowledge built into its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12155b10",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Variables\n",
    "\n",
    "First, we'll load our Hugging Face API token from the `.env` file. This token allows us to access Hugging Face's inference endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the token is loaded (shows only first few characters for security)\n",
    "token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "print(f\"Token loaded: {token[:10]}...\" if token else \"Token not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261e4a3",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Document Dataset\n",
    "\n",
    "We'll download a dataset of FAQ documents from various courses. Each document contains:\n",
    "- `question`: The FAQ question\n",
    "- `text`: The answer to the question\n",
    "- `section`: The section of the course\n",
    "- `course`: The course name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8e2ae0-da25-44e7-ae82-024173150a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57de60e5-b96c-499c-a7cf-0f30fc33b324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - Can I still join the course after the start date?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799651ff",
   "metadata": {},
   "source": [
    "Let's examine a sample document to understand the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7d0d18-5c07-4010-9f90-bbd021f110c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7c43acd9fc20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ada23",
   "metadata": {},
   "source": [
    "## Step 3: Create a Search Index with Minsearch\n",
    "\n",
    "**Minsearch** is a lightweight, pure-Python search library that implements text-based search.\n",
    "\n",
    "We'll create an index that:\n",
    "- Searches across `question`, `text`, and `section` fields\n",
    "- Filters by the `course` keyword\n",
    "- Uses boost weights to prioritize question matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff12ac",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Hugging Face LLM\n",
    "\n",
    "We'll use **Meta's Llama 3.2 3B Instruct** model through Hugging Face's Inference API. This allows us to:\n",
    "- Use powerful models without local GPU resources\n",
    "- Access models via simple API calls\n",
    "- Leverage Hugging Face's serverless infrastructure\n",
    "\n",
    "The model is wrapped with **LangChain** for easier message handling and integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db8de9",
   "metadata": {},
   "source": [
    "### Define Search Function\n",
    "\n",
    "This function retrieves the top 5 most relevant documents for a given query:\n",
    "- **boost**: Weights different fields (questions are 3x more important than sections)\n",
    "- **filter_dict**: Limits search to specific course\n",
    "- **num_results**: Returns top 5 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc5784e-6515-42e5-be62-8fb915df1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8834973",
   "metadata": {},
   "source": [
    "### Build Prompt for LLM\n",
    "\n",
    "This function creates a structured prompt for the LLM by:\n",
    "1. Taking the user's question\n",
    "2. Formatting the retrieved documents as context\n",
    "3. Instructing the LLM to answer based only on the provided context\n",
    "\n",
    "This ensures the LLM stays grounded in the retrieved facts and doesn't hallucinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Hugging Face model setup\n",
    "repo_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "client = ChatHuggingFace(llm=llm, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=prompt),\n",
    "        ]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829da69",
   "metadata": {},
   "source": [
    "### LLM Wrapper Function\n",
    "\n",
    "This function sends our prompt to the Hugging Face LLM using LangChain's message format:\n",
    "- **SystemMessage**: Sets the role/behavior of the AI assistant\n",
    "- **HumanMessage**: Contains the actual user prompt\n",
    "- Returns only the text content of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fcf4e",
   "metadata": {},
   "source": [
    "## Step 5: Complete RAG Pipeline\n",
    "\n",
    "Now we combine everything into a single RAG function that:\n",
    "\n",
    "1. **Retrieves** relevant documents using the search function\n",
    "2. **Builds** a context-rich prompt from the retrieved documents\n",
    "3. **Generates** an answer using the LLM\n",
    "\n",
    "This is the complete RAG workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka with Java, you need to execute the following command in your project directory:\\n\\n```sh\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n```\\n\\nIf you\\'re using Python and encounter the \"Module \\'kafka\\' not found\" error when trying to run `producer.py`, you should create a virtual environment, activate it, and install the required packages as per `requirements.txt`:\\n\\n1. Create a virtual environment (run only once):\\n   ```sh\\n   python -m venv env\\n   ```\\n\\n2. Activate the virtual environment:\\n   - On MacOS/Linux:\\n     ```sh\\n     source env/bin/activate\\n     ```\\n   - On Windows:\\n     ```sh\\n     env\\\\Scripts\\\\activate\\n     ```\\n\\n3. Install the necessary packages:\\n   ```sh\\n   pip install -r ../requirements.txt\\n   ```\\n\\nEnsure your Docker images are running if required. To deactivate the virtual environment when done, use:\\n```sh\\ndeactivate\\n```'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run kafka?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc25172",
   "metadata": {},
   "source": [
    "## Step 6: Test the RAG System\n",
    "\n",
    "Let's test our RAG system with some questions about the Data Engineering Zoomcamp course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can still enroll in the course even if it has already started. You are also eligible to submit the homework assignments. However, make sure to pay attention to the deadlines for turning in the final projects to avoid leaving everything until the last minute.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7922375",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "- A complete RAG (Retrieval-Augmented Generation) system\n",
    "- Text-based document search using Minsearch\n",
    "- LLM-powered question answering with Hugging Face\n",
    "\n",
    "### Why Hugging Face Endpoints?\n",
    "Instead of running models locally (which requires expensive GPUs), we use Hugging Face's **Inference API**:\n",
    "- ✓ **No local GPU needed** - runs on Hugging Face's infrastructure\n",
    "- ✓ **Fast deployment** - no model downloads or setup\n",
    "- ✓ **Cost-effective** - pay per request or use free tier\n",
    "- ✓ **Scalable** - handles multiple requests automatically\n",
    "- ✓ **Easy switching** - change models by just updating `repo_id`\n",
    "\n",
    "### Next Steps\n",
    "- Try `03_rag.ipynb` to see how **vector search** improves retrieval\n",
    "- Experiment with different questions\n",
    "- Try other Hugging Face models (see available models at https://huggingface.co/models)\n",
    "\n",
    "### Key Takeaways\n",
    "1. RAG combines retrieval + generation for better answers\n",
    "2. The LLM only sees context from retrieved documents (prevents hallucination)\n",
    "3. Hugging Face endpoints make using powerful models accessible without local hardware"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
